{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the tutorial materials\n",
    "\n",
    "+ Download zip:\n",
    "    In a browser go to https://github.com/aarontuor/tf_tutorials. Click clone or download button.\n",
    "+ Or clone the repo:\n",
    "    ```bash\n",
    "    $> git clone https://github.com/aarontuor/tf_tutorials.git\n",
    "    ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow virtualenv install\n",
    "\n",
    "```bash\n",
    "$> virtualenv --system-site-packages ~/tf_tutorials\n",
    "$> source ~/tf_tutorials/bin/activate\n",
    "$> export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0rc0-cp27-none-linux_x86_64.whl\n",
    "$> pip install --upgrade $TF_BINARY_URL\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter Notebook Install with Codefolding extension\n",
    "\n",
    "```bash\n",
    "$> pip install jupyter\n",
    "$> pip install https://github.com/ipython-contrib/jupyter_contrib_nbextensions/tarball/master\n",
    "$> jupyter contrib nbextension install --user \n",
    "$> jupyter nbextension enable codefolding/main\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matplotlib pip install\n",
    "```bash\n",
    "$> pip install matplotlib\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANTk pip install\n",
    "\n",
    "```bash\n",
    "$> pip install antk\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Introduction: Neural Network\n",
    "\n",
    "A neural network is just a parametric function. If you can find the right parameters, and have a large enough hidden layer size, a two layer neural network can \n",
    "approximate any function!\n",
    "\n",
    "Let $x \\in \\mathbb{R}^{1 \\times n}, W \\in \\mathbb{R}^{n \\times m},$ and $ U \\in \\mathbb{R}^{m \\times p}$. A two layer neural network is the parametric function $\\mathcal{q}: \\mathbb{R}^{1 \\times n} \\rightarrow \\mathbb{R}^{1 \\times p}$ where \n",
    "$\\mathcal{q} = g( U f(x W + b) + c)$, $U, W, b, c$ are the parameters to be learned, and the functions $g,h$ are model choices.\n",
    "\n",
    "![nnet graph](nnet_graph.png)\n",
    "\n",
    "Training a neural network involves a forward pass, which evaluates the function $q$ for a given set of parameters, and a backward pass which adjusts the parameters using gradient descent by way of the backpropagation algorithm, depending on how well $q$ approximates the training example targets. Tensorflow takes care of the math for the backward pass so we only need to worry about coding the forward pass for training.\n",
    "\n",
    "There are several choices for $f$. Below are a few:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhAAAAFkCAYAAABxWwLDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9v/P1Lx9hlvBOAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f59bc040e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "from antk.core.loader import DataSet\n",
    "\n",
    "x = np.linspace(-5, 5, 100) # 100 linearly spaced numbers between -10, 10\n",
    "\n",
    "# elementwise sigmoid\n",
    "f1 = 1/(1+np.exp(-x))\n",
    "\n",
    "# hyperbolic tangent: tanh\n",
    "f2 = (np.exp(2*x) -1)/(np.exp(2*x) + 1)\n",
    "\n",
    "# rectified linear units\n",
    "f3 = np.maximum(np.zeros(x.shape), x)\n",
    "\n",
    "plt.plot(x,f1, x, f2, x, f3)\n",
    "plt.gca().set_ylim(bottom=-1.5, top=1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Retrieve data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the computational graph of the function we want to make. Notice that $X$ is not a vector but a batch of vectors stacked together in a matrix, so we can send many vectors at a time through the neural net for training. \n",
    "\n",
    "![nnet graph](batch_nnet.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a graph and set as default\n",
    "g = tf.Graph()\n",
    "g.as_default() # Also better \"with tf.Graph().as_default() as g:\"\n",
    "\n",
    "# First make placeholders for inputs and targets\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784]) # The none dimension leaves us free to choose a batch_size at runtime\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10]) # Ten classes of digits\n",
    "\n",
    "# Now make trainable variables\n",
    "W = tf.Variable(0.01*tf.truncated_normal([784, 50], mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name='W'))\n",
    "U = tf.Variable(0.01*tf.truncated_normal([50, 10], mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name='U'))\n",
    "\n",
    "b = tf.Variable(tf.zeros([50])) # These don't need to be batchsize X 50 because of TF broadcasting\n",
    "c = tf.Variable(tf.zeros([10])) # Actually we really don't want the extra dimension because we want the bias to repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now compute q\n",
    "h = tf.nn.relu(tf.matmul(x, W) + b) # The hidden layer\n",
    "q = tf.nn.softmax(tf.matmul(h,U) + c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loss function\n",
    "cross_entropy = -tf.reduce_sum(y*tf.log(q))\n",
    "\n",
    "# Train step\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "\n",
    "# Evaluate\n",
    "correct_prediction = tf.equal(tf.argmax(q,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8715"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-7c6ac72f158b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_ys\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# release resources allocated to session also \"with tf.Session() as sess:\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys \n",
    "import time\n",
    "\n",
    "for i in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y: batch_ys})\n",
    "    sys.stdout.write('%s' % sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}))\n",
    "    time.sleep(.15)\n",
    "    sys.stdout.write('\\r')\n",
    "sess.close() # release resources allocated to session also \"with tf.Session() as sess:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A reusable nnet\n",
    "This code is perfectly usable but we've written a perfectly good neural network that only works with the vectorized mnist data (or some other 784 dimensional data set)! Let's make our neural network reusable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pass 1 at nnet classifier\n",
    "def nnet_classifier1(x, hidden_size, output_size, activation):\n",
    "    \"\"\"\n",
    "    First pass at a classifier\n",
    "    \n",
    "    :param x: Input to the network\n",
    "    :param hidden_size: Size of second dimension of W the first weight matrix\n",
    "    :param output_size: Size of second dimension of U the second weight matrix\n",
    "    \"\"\"\n",
    "    fan_in = x.get_shape().as_list()[1]\n",
    "    scale = 1.0/np.sqrt(fan_in)\n",
    "    W = tf.Variable(scale*tf.truncated_normal([fan_in, hidden_size], \n",
    "                                             mean=0.0, stddev=1.0, \n",
    "                                             dtype=tf.float32, seed=None, name='W'))\n",
    "    \n",
    "    scale = 1.0/np.sqrt(hidden_size)\n",
    "    U = tf.Variable(scale*tf.truncated_normal([hidden_size, output_size], \n",
    "                                              mean=0.0, stddev=1.0, dtype=tf.float32, \n",
    "                                              seed=None, name='U'))\n",
    "\n",
    "    b = tf.Variable(tf.zeros([hidden_size]))\n",
    "    c = tf.Variable(tf.zeros([output_size]))\n",
    "    \n",
    "    h = activation(tf.matmul(x, W) + b) # The hidden layer\n",
    "    return tf.nn.softmax(tf.matmul(h,U) + c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A better reusable nnet\n",
    "\n",
    "Our last attempt was better but we have restricted our neural net op to a neural network with one hidden layer. \n",
    "Let's make this network capable of arbitrarily depth and hidden layer sizes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pass 2 at nnet classifier\n",
    "def nnet_classifier2(x, layers=[50,10], act=tf.nn.relu, name='nnet'):\n",
    "    \"\"\"\n",
    "    Second pass at a classifier, eliminate repeated code. Bonus: An arbitrarilly deep neural network.\n",
    "    \n",
    "    :param x: Input to the network\n",
    "    :param layers: Sizes of network layers\n",
    "    :param act: Activation function to produce hidden layers of neural network.\n",
    "    :param name: An identifier for retrieving tensors made by dnn\n",
    "    \"\"\"\n",
    "    \n",
    "    for ind, hidden_size in enumerate(layers):\n",
    "        with tf.variable_scope('layer_%s' % ind):\n",
    "            fan_in = x.get_shape().as_list()[1]\n",
    "            scale = 1.0/np.sqrt(fan_in)\n",
    "            W = tf.Variable(scale*tf.truncated_normal([fan_in, hidden_size], \n",
    "                                                     mean=0.0, stddev=1.0, \n",
    "                                                     dtype=tf.float32, seed=None, name='W'))\n",
    "            tf.add_to_collection(name + '_weights', W)\n",
    "            b = tf.Variable(tf.zeros([hidden_size])) \n",
    "            tf.add_to_collection(name + '_bias', b)\n",
    "            x = tf.matmul(x,W) + b\n",
    "            if ind != len(layers) - 1:\n",
    "                x = act(x, name='h' + str(ind)) # The hidden layer\n",
    "            tf.add_to_collection(name + '_activation', x)\n",
    "    return tf.nn.softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminate boilerplate training code by making a reusable model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SimpleModel():\n",
    "    \"\"\"\n",
    "    A class for gradient descent training arbitrary models.\n",
    "    \n",
    "    :param loss: loss_tensor defined in graph\n",
    "    :param eval_tensor: For evaluating on dev set\n",
    "    :param ph_dict: A dictionary of tensorflow placeholders\n",
    "    :param learnrate: step_size for gradient descent\n",
    "    :param debug: Whether to print debugging info \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, loss, eval_tensor, ph_dict, learnrate=0.01, debug=False):\n",
    "        self.loss = loss\n",
    "        self.ph_dict = ph_dict\n",
    "        self.eval_tensor = eval_tensor\n",
    "        self.debug=debug\n",
    "        self.train_step = tf.train.GradientDescentOptimizer(learnrate).minimize(loss)\n",
    "        self.init = tf.initialize_all_variables()\n",
    "        self.epoch = 0.0\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(self.init)\n",
    "        \n",
    "    def train(self, train_data, dev_data, mb=1000, num_epochs=1):\n",
    "        \"\"\"\n",
    "        :param train_data: A DataSet object of train data.\n",
    "        :param dev_data: A DataSet object of dev data.\n",
    "        :param mb: The mini-batch size.\n",
    "        :param num_epochs: How many epochs to train for.\n",
    "        \"\"\"\n",
    "        while self.epoch < num_epochs:\n",
    "            self.epoch += float(mb)/float(train.num_examples)\n",
    "            new_batch = train.next_batch(mb)\n",
    "            self.sess.run(self.train_step, feed_dict=self.get_feed_dict(new_batch, self.ph_dict))\n",
    "            sys.stdout.write('epoch %.2f\\tdev eval: %.4f' % (self.epoch, self.evaluate(dev_data)))\n",
    "            time.sleep(.2)\n",
    "            sys.stdout.write('\\r')\n",
    "        \n",
    "    def evaluate(self, data):\n",
    "        \"\"\"\n",
    "        Evaluation function\n",
    "        \n",
    "        :param data: The data to evaluate on.\n",
    "        :return: The return value of the evaluation function in numpy form\n",
    "        \"\"\"\n",
    "        return self.sess.run(self.eval_tensor, feed_dict=self.get_feed_dict(data, self.ph_dict))\n",
    "                                 \n",
    "    def get_feed_dict(self, batch, ph_dict):\n",
    "\n",
    "        \"\"\"\n",
    "        :param batch: A dataset object.\n",
    "        :param ph_dict: A dictionary where the keys match keys in batch, and the values are placeholder tensors\n",
    "        :return: A feed dictionary with keys of placeholder tensors and values of numpy matrices\n",
    "        \"\"\"\n",
    "        \n",
    "        datadict = batch.features.copy()\n",
    "        datadict.update(batch.labels)\n",
    "                                 \n",
    "        if self.debug:\n",
    "            for desc in ph_dict:\n",
    "                print('%s\\n\\tph: %s\\t%s\\tdt: %s\\t%s' % (desc,\n",
    "                                                        ph_dict[desc].get_shape().as_list(), ph_dict[desc].dtype, \n",
    "                                                        datadict[desc].shape, datadict[desc].dtype))\n",
    "        return {ph_dict[key]:datadict[key] for key in ph_dict}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Testing SimpleModel class, and nnet_classifier operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     10
    ],
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from antk.core.loader import DataSet\n",
    "\n",
    "# Data prep ================================================================\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "train = DataSet({'images': mnist.train.images}, labels={'digits': mnist.train.labels}, mix=False)\n",
    "dev = DataSet({'images': mnist.test.images}, labels={'digits': mnist.test.labels}, mix=False)\n",
    "\n",
    "# Make graph ============================================================\n",
    "ph_dict = {'images': tf.placeholder(tf.float32, shape=[None, 784]),\n",
    "           'digits': tf.placeholder(tf.float32, shape=[None, 10])}\n",
    "\n",
    "prediction = nnet_classifier2(ph_dict['images'], \n",
    "                             layers = [50, 10], \n",
    "                             act = tf.nn.relu, \n",
    "                             name='nnet')\n",
    "\n",
    "# Loss function\n",
    "cross_entropy = -tf.reduce_sum(ph_dict['digits']*tf.log(prediction))\n",
    "\n",
    "# Evaluate\n",
    "correct_prediction = tf.equal(tf.argmax(prediction,1), tf.argmax(ph_dict['digits'],1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Make model\n",
    "model = SimpleModel(cross_entropy, accuracy, ph_dict, learnrate=0.01)\n",
    "\n",
    "# Train ================================================================\n",
    "model.train(train, dev, mb=100, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
